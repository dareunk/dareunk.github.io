---
title: "Information Retrieval(6) - dictionary compression"
categories:
    - Information Retrieval
tags:
    - Information Retrieval
date: 2023-11-05
toc: true
---

### Index compression

#### [ Lossless VS Lossy ]

- Lossless compression

    - 손실없는 압축

    - 모든 정보가 preservered

- Lossy compression

    - 몇몇 정보가 손실(discard)

    - 손실된 정보는 복원이 안됨

    - but Lossless compression에 비해 더 많은 압축이 가능 → 더 좋은 compression ratio
    
    - 손실되는 정보는 not important하기 때문에 별로 문제가 안됨 = “Lossy compression makes sense when the lost information is not important.
    
    - ex) case folding, stop words, stemming 등
    
    - ex) bmp → jpg, 음원 → mp3

- index compression에서 고려해야할 두 가지
  
    - 1) dictionary 공간
   
        - main memory에 유지할 수 있도록 충분히 작게 압축
   
    - 2) postings 공간
       
        - disk로부터 postings을 읽는 시간을 줄이기 위해 공간 압축
     
        - postings의 상당 일부분은 main memory에 저장(load)되어 있음 → for 빠른 검색을 위해

#### [ Index Compression 구분 ]

- Dictionary Compression

    - Dictionary as a string Method

    - Blocked Storage

    - Blocked Storage + Front Coding

- Postings Compression

    - Variable Byte Encoding

- Huffman Code

### Collection Statistics

#### [ 1. Vocabulary 추정 ]

- 각 문서집합마다 vocabulary size 제각각

- web scale에서는 사람,장소,상품 일므 등과 같은 정보들도 index에 포함되기 때문에 vocabulary가 굉장히 큼

- 이러한 dictionary는 memory에 load되어 있어야함

- 따라서, vocabulary(M)를 미리 예측해서 이것들을 압축시켜놓는 것이 필요!

#### [1.1 Heaps’ Law ]

> M = kT^b → logM = b logT + logk

- M: vocabulary size

- T: collection size, 특히 token의 개수 in collection

- k: 특정 상수 값 → 30 ≤ k ≤100

- b: 특정 상수 값  → b는 보통 0.5의 값

    - k와 b는 empirically하게 나온 값

- k는 collection의 nature(ex. text)과 이 nature이 어떻게 처리되었는지에 영향을 받음
  
    - case-folding, stemming: decrease k
  
    - inclusion of numbers and spelling errors: increase k

- 문서집합과 vocabulary 사이즈는 비례

    - 이때, 문서집합이 커질수록 vocabulary사이즈는 계속 커짐

    - 계속 커지다가 vocabulary의 사이즈는 일정(변화X)되는 것이 아님! 계속 커짐 = **no maximum vocab size**

- 따라서 dictionary를 compression하는 것이 좋음

    - document collection은 static하지 않고 dynamic

    - dictionary는 한계값이 없이 collection의 documents가 많아질수록 계속 커짐

    - 따라서 지금 당장 collection의 크기가 작더라도 시간이 지나면 점점 커질것이고, 이에 따라 dictionary도 점점 계속 커질 것임 → compression해야 함!

#### [1.2 Zipf’s Law ]

- 언어에서는 자주 사용되는 term이 있고, 자주 사용되지 않는 term이 있음

    - 정보검색에서는 자주 사용되는 term = useless → stop word

- term frequency와 function of rank(중요도, `~~등수~~`) 반비례 관계

> f_i = c/i → logf_i = logc - logi

- c: normalizing constant

- i가 클수록 f가 작음

    - i가 클수록 사용자가 적게 사용하는 단어들 → 중요도가 높음

    - 따라서 f가 작아짐

    - ex) f1 = 1000, f2=500, f3=333, f4=250 ,f5=200…..

- term frequency는 하한선을 가짐

    - i가 클수록 f는 작아짐

    - i가 커질수록 어느 순간 f는 하한선을 가지고 일정해짐

### Dictionary Compression

- Dictionary는 postings에 비해 크기 작은 편

- 그럼에도 불구하고 dictionary compression은 매우 중요

- 왜?

    1. query processing의 바른 처리를 위해 dictionary는 memory에 load되어 있음

    2. 빠른 start-up time을 위해 → dictionary가 memory에 load되는 시간을 단축

    3. 공용 자원인 memory를 다른 applications들과 더 많이 share할 수 있음


#### 1. compression을 하지 않은 경우 - fixed-width

- 한 term 당 28 bytes를 필요로 함

    - term(20bytes) + DF(int: 4bytes) + pointer to posting lists(pointer: 4bytes)

- RCV1: 400,000 terms X 28 bytes = 11.2MB

- **문제**

    - term들마다 길이가 다 다르기 때문에 적절한 bytes를 설정하는 것이 중요

    - RCV1기준으로 평균적인 terms의 bytes:7.5 → 많은 bytes in term column이 낭비!

    - 또한 20bytes를 초과하는 term들에 대한 처리가 불가능
    

### 2. Dictionary as a String

- dictionary를 하나의 긴 string으로써 저장

- 해당 term의 pointer는 string에서 term의 시작 지점을 나타냄

- 해당 term으로부터 다음 term의 pointer는 string에서 해당 term의 끝 지점을 나타

- 만약 dictionary에 40만개의 term이 존재

    - 40만 개의 term들은 하나의 긴 string을 pointing

    - 즉 40만 개의 pointer가 필요
    
![image](https://github.com/dareunk/dareunk.github.io/assets/83913407/925211c8-7613-4fe6-af70-7a1512162d7e)


**[ pointer to term의 bytes 구하기 ]**

- 1) Total String Length를 구하기

    - 전체 term의 개수 X 한 term당 평균 bytes 수

    - RCV1기준:  400,000 terms X 8(7.5) bytes/term = 3.2 MB

- 2) 즉 Total String Length는 곧 address space!

- 3) pointer to a string에 필요한 bytes 수를 계산

    - pointers는 이진수로 구성

    - size of pointers = log2의 address space

    - RCV1기준: log2(3.2) = 22 bits = 3 bytes

- DF(int: 4bytes) + pointer to posting lists(4bytes) + pointer to a string(3bytes) = 11 bytes / term

**[ dictionary을 위한 필요한 총 space 구하기 ]**

1. pointer to term의 bytes 구함

2. 평균적으로 dictionary에서 발생하는 term 당 필요한 bytes를 구함

3. In, dictionary as a string method에서는 하나의 긴 string을 필요로 하므로 이 공간도 함께 포함시킴

- ex) RCV1 기준

    - 첫 번째 방법

    - 평균적으로 한 term의 bytes는 8 바이트 → 즉, string에서 한 term당 차지하는 bytes수 8

    - 따라서 data structure에서 발생하는 11 bytes + 8 bytes = 19 bytes

    - 400,000 X 19 bytes = 7.6 MB
    
    - 두 번째 방법 - data structure에서 발생하는 space와 string에서 발생하는 space를 따로 구한 뒤 합하는 방법

    - 400,000 X 11 bytes = 4.4 MB

    - data structure를 위해 필요한 공간 : 4.4 MB + string을 위해 필요한 공간: 3.2 MB = 7.6 MB

    - 32 % 공간 절약

#### Blocked Storage

- Dictionary as a string method + terms들을 grouping 해줌 → grouping 된 term =  block 단위

- 해당 term의 길이를, 해당 term의 시작 위치에 추가해줌 in a string

- block의 size는 k

- 각 block에서 대표적으로 하나가 string으로 point

    - 끝 지점의 역할을 하는 pointer 기능 사라짐 like dictionary as a string method

    - 즉, 해당 term을 찾을 때 끝 지점을 파악하기 위해서 더 이상 다음 term의 pointer을 이용하지 않아도 됨

    - block단위로 string으로 point를 하기 때문에, 공간 save

**[ 얼마나 save 되었는가 ? ]**

1. block size가 k인 경우, 하나의 block에서 k-1 term pointers 제거됨

2. block를 이용하기 위해서는 k bytes를 필요

3. 따라서, (k-1) * 3 bytes - k bytes = 2k-3 bytes per block

    - 예) k=4라면, 5bytes per block saved

    - 예) RCV1기준

        - block의 개수 : 400,000/4 = 100,000 block

        - saved bytes = 5bytes/block X 100,000 block = 500,000bytes = 0.5MB

    - 0.5 MB가 전체 문서집합에서 saved

    - 7.6MB - 0.5 MB = 7.1MB

    - 37% save compared to fixed-width array(no compression,11.2MB)

**[ Block의 size ]**

- k을 높여주면, 그 만큼 한 block당 save되는 bytes는 커짐

- k을 높여준다 = compression의 정도를 높임

- 하지만, 검색이 너무 느려짐!!!

- 왜? k가 클수록 linear search에 가까워지기 때문에

    - 만약 k가 문서 집합의 개수만큼 설정되어있다면, 그것은 linear search로 검색을 진행하겠다는 것과 동일하게 search 동작

**Dictionary Search without Blocked Storage**

- Binary Search가 가능

- 알파벳 순으로 dictionary가 정렬되어 있음

- (1*1 + 2*2 + 3*4 + 4*1) / 8 = 21/8 = 2.6 번의 비교

![image](https://github.com/dareunk/dareunk.github.io/assets/83913407/5261ea56-2257-480b-a237-36fdf686b1f0)


**Dictionary Search with Blocked Storage**

- Binary Search로만은 불가능하고, linear Search도 함께 이용해야 함

- (1*1 + 2*2 + 3*2 + 4*2 + 5*1) / 8 = 3 의 비교

- 결론 : blocked stroage를 사용함으로써 dictionary의 공간은 saving할 수 있지만, 시간이 더 걸림

![image](https://github.com/dareunk/dareunk.github.io/assets/83913407/cf079a4e-cc0d-445f-abbc-c0dcdf9c6448)

#### 4. Blocked Storage + Front Coding

- term들을 sorted하면, 보통 term들은 동일한 prefix를 가지고 있음

![image](https://github.com/dareunk/dareunk.github.io/assets/83913407/ce0d0434-db13-4650-b6db-faa3df73e981)

- Blocked Storage + Front Coding을 통해 dictionary를 compression하면 사전의 크기를 예측하는 것이 불가능 → 비슷한 단어가 몇 개인지 다 파악할 수 없기 때문에

- but, blocked storage + front coding이 가장 compression ratio가 좋음

#### Compression ratio 비교

Blocked Storage + Front Coding > Blocked Storage > Dictionary as a string > no compression
