---
title: "Information Retrieval(3)"
categories:
    - Information Retrieval
tags:
    - Information Retrieval
date: 2023-10-16
toc: true
---

Tokenization/Normalization/Stemming/Lemmatization/Phrase query처리방식-biword index,positional index 


### Tokenization

> 각각의 단계는 기계학습의 일종으로 classification problem이라고 할 수 있음


> /하지만, 실제로는 heuristically하게 처리하고, heuristically하게 처리할 때 그 정확도가 낮은 경우에 한에서 기계학습을 많이 이용


> tokenization은 그렇게 간단하지 않음
> 
> - whitespace 단위로 자름
> - punctuation(.) 버림
> - apostrophe(ex. jin’s) 처리
>     - ex) query: O’jins  → terms’ index: ojins, ojin, o’jins
>     - query와 term을 맞춰주는 과정이 필요

#### 1. **Document format**

    - pdf/word/excel/html

    - 문서들 마다 각기 다른 형식 → 각각 다른 라이브러리를 이용해 다른 처리를 해서 tokenization을 해야함

    - 확장자로 식별하지 않음 ! → 부정확하기 때문

        - 문서 내용을 통해 pdf, exe인지 format을 확인 (기계학습의 일종)

    - 각각의 document format은 다른 고유한 magic number(hex digits)을 가짐

        - 확장자는 참고용도, 확장자가 없어도 그 document의 format식별이 가능함

        - 항상 특정 확장자는 동일한 magic number을 가짐 → 서로 약속해놓음

        - 이렇게 magic number을 이용하면 기계학습이 필요없음

#### 2. **Language(identification)**

    - 문서가 어떤 언어로 작성되었는지 **언어 식별**

    - 언어마다 tokenization의 난이도 다름

    - short character subsequences를 이용
    
        - ex) 나는 학생이다 → sksms gkrtoddlek

        - ex) i’m a student → ㅑ’ㅡ ㅁ ㄴ셩둣

    - Unicode에서는 언어 별로 영역이 다름 → 언어마다 패턴이 다름

    - 필요한 이유: tokenization은 **language-specific**하므로

        - language의 종류마다 다른 module필요
        
        - ex) 독일어

            - 독일어는 compound nouns를 분리하지 않음 → 다 이어씀
            
            - 독일 검색 시스템은 compound splitter module이 필요

            - + 중국어의 복합명사를 띄어쓰는 splitter module이 필요

        - ex) 일본어

            - 일본어도 띄어쓰지 않음

            - 히라가나, 가타카나를 기준으로 나눠 자를 수 있음 → 중국어보다는 구분 쉬움

            - but 항상 사용자가 정확하게 히라가나와 가타카나를 나눠 query를 작성한다는 guaranteed 없음

            - 만약 사용자가 그냥 다 히라가나로 작성하는 경우? → 이런 경우는 더 복잡해짐

        - ex) Arabic(or Hebrew)

            - 기본적으로 오른쪽에서 왼쪽으로 씀(보통과 반대됨)

            - but 숫자와 같은 특정 items에 대해서는 왼쪽에서 오른쪽으로 씀

            - 단어들은 떨어져있음, 하지만 letter forms이 complex ligatures임

            - 즉, visual presentation은 복잡하지만, Unicode로 저장되는 형태(stored form)은 간단한 편

#### 3. **Character set(code)**

    - ex) KSC5601 vs UTF-8(UNICODE)

        - UTF-8로 작성된 한글 KSC5601로 읽으면 제대로 읽지 못함

    - Character set도 language identification과 유사한 방식으로 식별


####  Tokenization’s issues 


1. hyphen

- hyphen(-)을 어떻게 처리해야 하는가에 대한 일관성 문제 발생

- ex) hello-world

    - helloworld로 하나의 token으로 볼 것인가 아니면 hello와 world라는 두 개의 token으로 볼 것인가?

    - 일관성 문제가 발생

- 만약 hyphenation에 phrase index까지 더해지면 문제가 더 complex

    - ex) San Francisco-Los Angeles


1. 그 외 

- 이것들을 하나의 token으로 볼 것인가 아니면 여러 개의 token으로 나눠 볼 것인가?

| 10/16/2023 |
| --- |
| Oct.16.2023 |
| 55 B.C. |
| (800)-123-5678 |
| 210.123.45.67 |
| hello@sungshin.ac.kr |

1. 설명

- 너무 복잡하니 그냥 token으로 취급하지 않으면?

    - 이런 token들은 query에서 굉장히 중요한 역할

- 그럼 경우를 다 나눠 token으로 취급하면?

    - vocabulary의 size가 증가 → 결국 index의 size가 너무 커짐

### Stop words 불용어

- 너무 자주 쓰이는 단어

- 적은 의미를 가진 단어 = little semantic content ↔ content word(ex. apple)

- function word: 문법적으로 기능하고 의미가 별로 없음

    - ex) the, a, and, to, be

- top 30개의 terms들의 posting들 중 ~30%를 차지할 정도로 많은 비율

####  stop list 

- stop words를 모아둔 list

- dictionary로부터 이 stop list에 있는 단어들을 제외

- → 상당히 많은 수의 posting들을 줄일 수 있음 (~30%)

#### Trends

- stop list의 사이즈가 작음

- 이유

    - 1) 찾고자 하는 문서와 관련 없는 문서들이 검색될 확률 높음 → precision이 낮음

    - 2) 사용자의 본래의 query를 너무 손상시킬 가능성이 있음

- Good compression techniques

    - stop list의 사이즈 확대

    - inverted index의 사이즈가 줄어들기 때문에 공간적 이점

    - 단점

        - query손상

        - precision낮음

- Good query optimization techniques

    - stop list에서 stop words를 적게 설정

    - 압축시켜서 index에 저장(vector space model) → 공간을 조금이나마 줄임

    - 예전보다 기술이 많이 좋아져 stop words를 적게 설정해도 문제 안됨

    - query를 good compression techniques보다 많이 보존하기 때문에 검색까지 걸리는데 시간이 많이 소요

        - 이를 해결하기 위해 이들의 가중치(weight)를 줄여 검색 속도 증가

    - query에서 stop words는 포함하되 보다 중요한  content word보다 가중치를 좀 낮게 설정 → 하나의 query안에서 가중치(weight)를 달리 설정

### Normalization

- query terms를 index상의 동일한 형태의 terms으로 만들어주는 normalization 과정이 필요

#### Mapping rules

- remove dots, hyphens …

- ex) U.S.A = USA, anti-virus = antivirus

#### Thesaurus 유의어 사전

- handle synonyms, hypernym(상위어), hyponym(하위어)

- Theasurus 제작 방법

    - 1) hand-constructed : 사람이 직접 thesaurus 생성

    - 2) Auto-generated: text분석 통해 자동화 방법으로 thesaurus 생성

- Theasurus를 통한 index 접근 방법

    - 1) query 정규화 처리를 통해 class name(대표이름)으로 term 변경

        - 대표이름으로 index에서 문서 찾음

    - 2) Multiple indexing

        - 유의어들의 document들을 다 포함한 유의어들을 모두 index에 추가 → 유의어들끼리 posting들의 구조 동일

            - index의 사이즈가 커짐
            
            - query 정규화 처리가 필요없음 → query 처리 시간이 빨라짐 → 사용자의 만족감 증가(반응시간 증가)

        - 하나의 term을 입력하더라도 아무 작업없이 유의어들도 함께 출력

    - 3) Query expansion

        - 유의어들이 모두 index에 추가 → but, 유의어들을 각각의 posting들을 가짐

        - 나중에 이들을 merge해서 사용자에게 보여줌(중복되는내용은 제거해줘야함)

        - thesaurus를 통해 query를 확장시켜줌 → expanded query

        - 검색시간이 오래걸림 → 가장 오래걸리는 방법
        

#### language-specific

- normalization도 tokenization처럼 language-specific함 = **language-specific normalization**

- ex) Accents, diacritics

- case-folding

    - 모든 letters을 소문자로

    - 문제 : unintended query expansion problem

        - 대문자의 A와 소문자의 A의 의미가 다른 경우

- 그러나 이런 것들을 모두 고려한다하더라도 사용자가 correct capitalization을 입력한다는 보장이 없음

- idiosyncratic

    - ex) 날짜의  표현

### Stemming vs Lemmatization

term을 형성하는 과정에서 stemming을 할 것인지 lemmatization을 할 것인지를 결정 → 둘 중 하나 선택 필요

#### Stemming

- 자름

- language-specific함

    - ex) crude affix(가공되지 않은 천연의 접사)

    - ex) automates, automatic, automation → automat

- exact stemmed form은 중요하지 않음

    - 이것보다는 앞서 봤던 normalization에서의 class name 생성을 어떻게 할 지가 IR에 더 영향

- 대표적인 알고리즘 : **porter’s algorithm**

    - 영어를 stemming할 때 가장 흔하게 사용되는 알고리즘

    - porter’s algorithm을 통해 나온 term들의 형태는 실제로는 사전에 존재하지는 않는 단어들

| sses | ss | caresses → caress |
| --- | --- | --- |
| ies | i | ponies → poni |
| ss | ss | caress → caress |
| s | 없앰 | cats → cat |
| (m>1)ement | 없앰 | replacement → replac |
| (m≤1)ement | ement | cement → cement |

#### Lemmatization

- base form, 기본 단어 형태로 만듦

    - base form의 term이란? 사전에 있는 단어들 ⇒ dictionary word form(lemma)

- morphological analyzer(형태소 분석기)를 사용하기도 함

| am,are,is | be |
| --- | --- |
| car,cars,car’s,cars’ | car |
| operate,operating,operates | operate |
| operative, operatives | operative |
| operation | operation |
| operational | operational |

#### 차이

- stemming은 recall은 증가시키지만 precision은 낮춤

    - 다른 형태의 동일한 언어를 같은 방식으로 처리해주기 때문에 포함되는 문서들이 많음

    - ex) operate, operating,operates,operative,operatives,operation,operational → operat

    - term을 형성하는데 시간이 적게 소요되어 index 구축의 소요를 줄여줌 → stemming 많이 사용됨

- Lemmatization이 IR에서는 가장 큰 이점을 지님

    - 특히, 영어에서

    - 정확한 base form을 추출하기 때문에 시간 소요가 많은 편


### Multilingual documents

- 다른 다양한 languages로부터의 document collections

1. document가 어떤 language로 구성되어 있는지를 감지

2. language-specific normalization과 stemming or lemmatization해줌

3. language identification tag를 index시 함께 넣어줌 

    - dictionary의 size가 커짐

![image](https://github.com/dareunk/dareunk.github.io/assets/83913407/82294647-f019-4c21-b40a-f28b8dff9900)


### Faster posting merge - skip pointers/skip lists

#### skip pointers

- skip pointers을 설정 ex) skip pointers=3

- skip pointers은 오직 AND query에서만 효과, OR query에서는 도움 안되는 것 당연!

- 원래는 search되어야하는 result를 생략해버릴 수도 있는 단점이 존재

- skip pointers의 설정은 empirically(실험적)으로 결정

- Tradeoff

    - More skips

        - shorter skip spans

        - skip pointers들의 비교가 많아짐

        ![image](https://github.com/dareunk/dareunk.github.io/assets/83913407/4c85699c-05ea-4784-a710-18d9706cd77e)

    - Few skips

        - long skip spans

        - skip pointers들의 비교가 적음

        - skip 가능성이 적음

        
     ![image](https://github.com/dareunk/dareunk.github.io/assets/83913407/f549dcfd-7902-471f-a3c4-495dce3cd02e)
        

### Phrase queries

- web queries의 10%는 phrase queries

- 기존의 inverted index 형태로는 phrase queries처리가 불가능

#### [ Solution1 - Biword index ]

- consecutive pairs form으로 만듦 → biword

- 이 biword들을 dictionary term으로 추가 ex) friends romans

- 장) phrase queries 바로 처리가 가능 → 빠름, 반응시간이 좋아 사용자의 만족도 올라감

- 단점

    - 1) index의 크기가 굉장히 커짐

    - 2) false positive 발생 가능성이 있음

    - 3) biword의 경우의 수가 너무 많음 → 기하급수적으로 증가

    - 4) **proximity search 불가능** ↔ positional index와의 차이

        - ex) 10개의 단어 : 10 * 10

- biword index는 standard solution 아님

![image](https://github.com/dareunk/dareunk.github.io/assets/83913407/be4e877b-45db-47c3-810a-9a7e4ee5d85f)


- longer phrase의 경우

    - longer phrase의 경우 biword로 쪼갠 후 boolean query로 만듦(AND 연산)

    - ex) stanford university palo alto

        - stanford university AND university palo AND palo alto

    - 장) 균등하게 좋은 결과를 나타낼 수 있음

    - 단) 가끔 잘못된 결과를 도출할 수 있음 → false positive

        - 실제로 틀린 결과를 맞다고 주거나 실제로 맞은 결과를 틀렸다고 줌


#### [ Solution2 - positional index ]

- 가장 흔한 solution

- position information을 index에 추가 → 각 document들은 position lists를 point

- index의 크기가 커지지만, phrase queries를 처리하기 위해서는 index크기 증가의 단점을 감수

- flexable함

- 그냥 AND 연산을 할 때는 position information이 필요없음 → phrase queries를 처리하기 위해서는 각 문서의 position information을 이용

- proximity queries 처리 가능

    - ex) /3, /s, /p

- positional index size는 평균적으로 document들의 size에 영향을 받음

    - 웹에서의 경우: < 1000 terms

        - 한 문서에서 1000개의 terms에 대한 position information의 list가 만들어짐

        - positional index가 상대적으로 짧음

    - 책에서의 경우: 100,000terms

        - positional index의 size가 굉장히 큼

- positional index의 size은 보통 non-positional index에 비해 2-4배 가량 큼

- positional index의 size는 보통 문서의 크기의 35-50프로 가량 차지

- 따라서, positional index를 하기 위해서는 여러 대의 컴퓨터가 필요하고 이를 clustering함


#### [combination schemes]

- hybrid한 방식

- 결론: 사람들이 많이 사용하는 phrase query에 대해서는 효율성을 위해 phrase index나 biword index방식을 이용, 나머지는 positional index방식을 이용

- common query에 대해서 positional lists를 merge에서 검색결과를 보여주면 비효율적 → 처리 시간 오래걸림

- ex) “Michael Jackson”, “Britney Spears”

- 이러한 경우에는 phrase를 index에 추가하거나, biword로 추가