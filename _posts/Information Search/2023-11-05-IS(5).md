---
title: "Information Retrieval(5)"
categories:
    - Information Retrieval
tags:
    - Information Retrieval
date: 2023-11-05
toc: true
---

Index Construction


### Reuters RCV1

- 작은 문서 집합 → 약 800,000 문서(1GB)

- 1년동안의 뉴스 기사를 모아놓은 것

- 오픈되어 있음

- 넓은 범위의 international topics들을 가지고 있음

- scalable index construction algorithm을 제작할 때, 초기 기초 문서집합으로 사용

### Scalable (Index Construction) Algorithm

- data의 scale을 키우더라도 잘 적용되는 algorithm을 scalable하다라고 말할 수 있음

### Index Construction

#### [ 종류 ]

- Sort-based Index Construction : non-scalable

- Scalable Index Construction

    - BSBI: Blocked Sort-Based Indexing

    - SPIMI: Single-Pass In-Memory Indexing

    - Distributed Indexing

### Sort-based Index Construction

- **In-memory sort-based index construction = non-scalable**

- 앞서 배웠던 indexr가 index를 구축하는 방식이라고 보면 됨

- ( term, doc#)로 쌍을 만듦

- term을 기준으로 sorting → sorting시간 오래 걸림

    - 문자열이 길면 sorting 더 오래 걸림

    - sorting의 대상(term)이 많아지면 sorting 방식에 따라 sorting의 시간 달라짐

- 이를 해결하기 위해 term ID 사용

    - term은 각기 고유한 term ID를 가짐

    - (term#, doc#) 이용

- (term#, doc#) → 4bytes + 4bytes = 8bytes

    - 예) RCV1을 기준으로 100,000,000 pairs X 8 bytes = 0.8GB

    - 0.8GB의 data를 memory에 load시키고 sort 진행

    - 0.8GB에 대해서는 가능하겠지만, 만약 data의 사이즈가 더 커진 큰 문서집합에 대해서는 불가능함 → non-scalable

    - 따라서 위와 같은 경우에는 data의 일부는 memory에, 또 일부는 disk에 저장해놓고 왔다갔다 하면서 sorting을 진행해야함 → 시간 너무 오래걸림
    

### Scalable Index Construction

#### BSBI: Blocked Sort-Based Indexing

- sorted-based index construction의 일종 → but, scalable함

- 아무리 큰 문서집합에 대해서도 block단위로 sorting을 진행하기 때문에 처리가 가능함

- 모든 block들에 대한 index의 merge를 메모리에 다 load해서 하면 문제
 
- index들 중 일부분 merge진행 후 disk에 write → 이런 식으로 반복함

- 결론적으로 sort와 merge를 block단위로  진행하여 하나의 큰 inverted index로 만들어주기 때문에 scalable하게 index를 구축할 수 있음

**[ 과정 ]**

- 전체 큰 문서집합에서  하나의 block이 꽉 찰 때까지 메모리에 (term#, doc#) pairs를 모음

- 이 block으로 index 만듦
   
    - sorting by term ID
    
    - 같은 term ID가진 모든 pairs들을 posting list로 만듦
  
    - 즉 block단위로 inverted index를 생성

- block단위의 inverted index를 disk에 저장

- 큰 문서집합에 대해 위의 처리를 끝낼 때 까지 block단위 sorting을 반복한 후에 block단위로 merge를 진행
    
    - 두 개의 sorted된 block들을 disk로부터 읽어와 memory에서 merge
   
    - merged sorted block을 다시 disk로 write
   
    - 최종적으로 merge를 진행하면 하나의 큰 inverted index가 생성

**[ n-way merge ]**

- 두 개의 파일(block)을 대상으로 merge을 하는 것 아님

- 특정 사이즈의 chunk을 각 block으로 부터 memory로 읽어와 merge를 진행함 → 이때 동시에 열 수 있는 파일이 10개라면 10개의 block으로부터 chunk단위(starts at lowest term#)로 merge를 동시에 진행할 수 있음

- 시스템마다 simultaneously 하게 열 수 있는 파일의 개수 한정 → but 보통 3개 이상

**[ 문제 ]**

- dictionary를 위한 term to termID mapping 을 memory에 load시켜놔야 함

    - term ID dictionary는 모든 block들이 공유해야 하기 때문에 ‘하나’의 큰 dictionary로 되어 있음

    - 또한 이 dictionary는 index construction과정에서 항상 memory에 load되어 있어야 함

- dictionary사이즈는 점점 커짐

- 이를 해결하기 위해 termID가 아닌 term을 그냥 사용해 (term, doc#)으로 pairs을 생성할 수 있음

    - 여전히 scalable한 indexing 구축 가능

    - intermediate files의 크기가 굉장히 커짐

    - index construction 속도가 굉장히 느림

        - 중간파일의 크기가 크기 때문에 disk에서 memory에 올리거나, memory에서 disk wirte할 때 걸리는 시간이 오래걸림

        - memory에서 중간파일(block)을 sorting하는데 걸리는 시간이 비교적 더 증가

#### SPIMI: Single-Pass In Memory Indexing


- hash를 이용

- sorting을 하지 않음

- term을 term ID로 변환하지 않음

    - 즉,  block들을 통합하는 하나의 거대한 term to term ID mapping dictionary가 필요 없음

- term을 hash function을 이용해 처리하면 unique한 값이 나옴 → 이 값에 term과 그 term이 pointing하는 posting lists를 구축

    - 즉 term은 고유한 hash값을 가짐

    - 해당 hash값에 방에는 그 term을 가진 documents들의 posting lists가 존재

- 계속 이렇게 추가가 되기 때문에 ‘방’의 사이즈를 사전에 추측하는 것이 불가능

    - 방은 계속 indexing을 하는 과정에서 커짐 →동적할당이 필요

    - 1) 연결리스트 형태로 구성

    - 2) 배열로 구성

        - 단점: 배열은 동적으로 공간 할당이 불가능, 따라서 이러한 경우에는 size를 증가한(x2) 새로운 배열을 생성한 후 기존의 배열 안의 데이터를 새로운 배열에 copy해줘야 함 → copy이후 기존 배열은 삭제

- 각 block마다 separate dictionary, separate inverted index들을 생성

**[ merge ]**

- separate inverted index로 존재하면 sorting을 안해서 좋겠지만, 결국에는 나중에 검색결과를 반환하기 위해서는 and 연산처리는 해줘야 함→ 그렇기 때문에 적정범위 내(너무 크지 않은 index들 범위)에서는 merge를 해주는 것이 좋음

- 이렇게 separate dictionary(inverted index)를 merge하기 위해서는 sorting이 필요

- block마다 같은 term들은 다른 위치에 존재

- 만약 sorting 을 하지 않고 hash function으로 하나하나 각 dictionary을 읽으면 시간이 너무 오래걸림

 

```cpp
SPIMI-INVERT(token_stream)
	output_file = NEWFILE()
	dictionary = NEWHASH()
while(free memory available)
	do token <- next(token_stream)
		if term(token) is not in the dictionary
			then posting_list = ADDTODICTIONARY(dictionary, term(token))
		else posting_list = GETPOSTINGLISTS(dictionary, term(token))
if full(posting_list)
	then posting_list = DOUBLEPOSTINGLIST(dictionary, term(token))
ADDTOPOSTINGLIST(posting_list, docID(token))
sorted_terms <- SORTTERMS(dictionary)
WRITEBLOCKTODISK(sorted_term,dictionary,output_file)
return output_file
```

#### Distributed Indexing

- web-scale indexing

    - web에서 가져온 큰 스케일의 문서들을 처리

    - 앞의 SPIMI도 scalable해서 가능하지만, 시간이 오래 걸림

- 여러 대의 컴퓨터를 이용하여 각 컴퓨터들에 나눠서 indexing 진행

    - 하나의 컴퓨터로는 web-scale의 큰 문서집합에 대해 index construction을 진행하기가 어려움

    - must use a distributed computing cluster

- 또한, 컴퓨터는 기계적 결함에 의해 망가지거나, 종료될 수 있기 때문에 여러 대의 컴퓨터를 이용하여 indexing을 하는 것이 좋음

- 두 가지 방법 존재

    - 1) Term-partitioned indexing

    - 2) Document-partitioned indexing → 가장 많이 사용되는 search engines

- MapReduce를 베이스로 함

**MapReduce**

- 분산컴퓨팅의 방법론 중 하나 → 일반적인 방법론

- large computer clusters을 위해 디자인 됨

    - → 이때 cluster안의 각 node들은 컴퓨터

    - 이 node들은 언제든 고장날 수 있음

    - 각 node들에게 임무를 부여하는 역할을 하는 노드 = master node

    - n splits

        - split은 각 노드들에게 주어지는 할당량 (in a short time) 단위

- 처리 방식: two parallel tasks

**[ two parallel tasks ]**

이때, parsers과 inverters들의 역할이 딱 구분되는 것은 아님 ← master node의 권

1. input documents가 들어오면 n splits으로 나눔 

    - 각 node가 빠르게 split을 처리할 수 있도록 적절히 작게 task를 나눠서 분배

2. 각 node들( parsers )이 각 split들을 처리함

3. ( term, doc#) pairs이 생성되고 이것들은 j partition으로 구성

    - j개 만큼의 부분으로 나뉘어짐

    - j개 만큼의 partition을 가진 n개의 segment들이 생성됨

    - ex) j=3인 경우

        - 한 segment에 a-f, g-p, q-z의 3개의 partition이 생성

4. inverters에 의해서 통합됨 

    - 각 partition으로부터 (term,doc#)을 모아 통합

    - posting list로 구성(sort and write)

**1. Term-partitioned Indexing**

- 각 분산된 컴퓨터들이 partition된 term들을 처리

- partition된 term들마다 사람들이 더 많이 찾는 용어가 있고 아닌 용어가 있음

- partition된 term들마다 더 많은 documents를 포함하고 있는 경우가 있고 아닌 경우가 있음

- → 예측이 어려움

- → 따라서, 적절하게 balance load를 하는 것이 어려움 in term-partitioned indexing

**2. Document-partitioned Indexing**

- 일반적으로 사용되는 방식

- 같은 term이더라도 posting lists가 긴 경우들이 존재 → 이 경우에 유용

- posting lists를 나눠 여러 컴퓨터에 분산 처리를 함

- ~~query를 처리할 때 and 연산에 의해 발생하는 처리속도를 빨리할 수 있음~~

- 하나의 query에 대해 여러 대의 search node들이 처리를 해야하지만, balance와 re-balance load를 하는데 있어 효과적!

### Dynamic Indexing

- 문서집합은 static하지 않고 dynamic함

    - 계속 변함

    - 삭제, 수정, 삽입

    - 즉, dictionary와 posting lists가 수정될 경우를 처리하는 것이 필요함

- 만약 문서집합의 변화로 인해 dictionary와 posting list를 update, 즉 index를 변경하는 동안에는 검색이 불가능함 → 문제 발생

#### [ simplest approach ]

- 두 개의 인덱스가 존재

    - 하나의 큰 main index

    - 작은 auxiliary index (in memory)

- dynamic한 변화를 반영함과 동시에 사용자가 계속 검색을 가능하게 하기 위해서는 보조인덱스에 변경사항을 반영해놓는 것이 좋음 → auxiliary index

- main index에는 변경사항이 반영이 안 되어있기 때문에 사용자에게 검색 결과를 반환할 때는 보조 인덱스를 함께 반영하여 결과를 반환해줘야 함

- 문제

    - 변경사항이 너무 많아져 auxiliary index가 너무 커지게 되면 이것을 main index로 merge 시켜야 함

    - merge하는 동안은 검색 서비스가 불가능

    - main index에 merge되는 크기 단위를 크게 설정해 너무 빈번하게 merge가 되지 않도록 함

    - → 그럼에도 simplest approach를 이용하면 ‘main index’로의 merge 빈도수가 많은 편 → poor performance for information retrieval

#### [ Logarithmic Merge ]

- smallest index(Z_0) → memory

- larger index(l_i) → disk

- disk에 있는 larger index와 새로운 auxiliary index(smallest index)의 크기가 같으면 merge 진행 → merge 진행 시 크기가 2배씩 커짐

    - merge 시 Z_i의 temporary index가 disk에 생성

    - 이 temporary index가 l_i의 permanent index가 됨

- 갈수록 큰 크기의 index의 merge빈도가 작아짐

- 큰 index의 merge 빈도가 작아짐

- 작은 index의 merge 빈도는 많음

- 문제 ) 검색결과를 처리하기 위해서는 이렇게 index들을 다 읽어내야 함 → 처리 속도 오래걸림

    - 그래도 변화된 결과를 검색 결과에 반영하고, main index로의 merge가 적게 일어나 검색에 영향을 안 미치므로 이러한 단점은 감수해야 한다고 생각

    - 해결방안: 주기적으로 scratch로부터 index를 reconstruct해줌
    
        - merge를 시켜 오래된 index를 삭제하고 새로운 좀 더 큰 index를 생성 → index를 and처리해주는 과정을 줄여나갈 수 있도록 주기적으로 reconstruct처리를 해준다고 생각하면 됨

```markdown
LOGARITHMICMERGE()
Z0 <- 0 // ZO is in the memory
indexes <- 0
while true
do LMERGEADDTOKEN(indexes,Z0,GETNEXTTOKEN())

LMERGEADDTOKEN(indexes,Z0,token){
Z0 <- merge(Z0, {token}) // Z0이 꽉 차기 전까지는 새로운 변경사항을 Z0에 기입
if |Z0| = n // Z0이 꽉 찬다면.. 
	then for i<-0 to infinite
		do if l_i is element for indexes // Zi와 같은 크기의 li가 disk에 존재한다면
					then Z_i+1 <- MERGE(l_i,Z_i)
						(Z_i+1 is a temporary index on disk)
							indexes <- indexes - {l_i}
			// 그게 아니라면
			else l_i <- Z_i (Z_i becomes the permanent index l_i)
					indexes <- indexes 합집합 l_i 
					break;
			Z0 = 0 
		
}
```